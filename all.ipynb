{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ff4ac-94cf-4639-9dd4-9620ac08cbd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/HP/Desktop/ml/project1\n",
      "[nltk_data]     classification/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:/Users/HP/Desktop/ml/project1\n",
      "[nltk_data]     classification/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:/Users/HP/Desktop/ml/project1\n",
      "[nltk_data]     classification/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:/Users/HP/Desktop/ml/project1\n",
      "[nltk_data]     classification/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Extract URLs from the text\n",
    "def extract_urls(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    return urls\n",
    "\n",
    "# Clean email body\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'>+=+=+=+=+', '', text)  # Remove separators\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text_tokens = text.split()\n",
    "    filtered_words = [word for word in text_tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def create_preprocessor():\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('body', Pipeline([\n",
    "                ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "                ('tfidf', TfidfVectorizer(min_df=1, max_df=0.9))\n",
    "            ]), 'body'),\n",
    "            ('extracted_urls', Pipeline([\n",
    "                ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ]), 'extracted_urls'),\n",
    "            ('message_length', Pipeline([\n",
    "                ('length', FunctionTransformer(lambda X: np.array(X.apply(len)).reshape(-1, 1)))\n",
    "            ]), 'body') \n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Train models\n",
    "def train_models(X_train, y_train):\n",
    "    classifiers = {\n",
    "        'RandomForest': RandomForestClassifier(),\n",
    "        'NaiveBayes': MultinomialNB(),\n",
    "        'SVM': SVC(probability=True),\n",
    "        'LogisticRegression': LogisticRegression()\n",
    "    }\n",
    "\n",
    "    models = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', create_preprocessor()),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "        model.fit(X_train, y_train)\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "# Evaluate models and display results\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        print(f\"Results for {name}:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "        results[name] = {\n",
    "            'y_pred': y_pred,\n",
    "            'roc_auc': roc_auc,\n",
    "            'y_prob': y_prob\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Plot ROC curves\n",
    "def plot_roc_curves(results, y_test):\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    for name, res in results.items():\n",
    "        if res['y_prob'] is not None:\n",
    "            fpr, tpr, _ = roc_curve(y_test, res['y_prob'])\n",
    "            plt.plot(fpr, tpr, label=f\"{name} ROC AUC: {res['roc_auc']:.2f}\")\n",
    "    plt.legend()\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "def plot_confusion_matrices(results, y_test):\n",
    "    fig, ax = plt.subplots(1, len(results), figsize=(15,5))\n",
    "    for i, (name, res) in enumerate(results.items()):\n",
    "        cm = confusion_matrix(y_test, res['y_pred'])\n",
    "        sns.heatmap(cm, annot=True, cmap=\"Oranges\", fmt='g', ax=ax[i])\n",
    "        ax[i].set_title(f\"{name} Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    \n",
    "    # Set the NLTK data download path\n",
    "    nltk_data_path = 'C:/Users/HP/Desktop/ml/project1 classification/nltk_data'\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    \n",
    "    # Ensure necessary NLTK resources are downloaded\n",
    "    nltk.download('punkt', download_dir=nltk_data_path)\n",
    "    nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "    nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "    nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "    \n",
    "    # Load the dataset\n",
    "    file_path = 'C:/Users/HP/Desktop/ml/project1 classification/CEAS_08.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    # Load data\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # Data cleaning and preprocessing\n",
    "    df['extracted_urls'] = df['body'].apply(extract_urls)\n",
    "    df['body'] = df['body'].apply(clean_text)\n",
    "    \n",
    "    # Features and labels\n",
    "    X = df[['body', 'extracted_urls']]\n",
    "    y = df['label']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train models\n",
    "    models = train_models(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    results = evaluate_models(models, X_test, y_test)\n",
    "    \n",
    "    # Plot ROC Curves\n",
    "    plot_roc_curves(results, y_test)\n",
    "    \n",
    "    # Plot Confusion Matrices\n",
    "    plot_confusion_matrices(results, y_test)\n",
    "    \n",
    "    # ---- New Data for Testing ----\n",
    "    print(\"\\n--- Testing on New Data ---\")\n",
    "    \n",
    "    # Example new data (replace this with actual new email data)\n",
    "    new_data = pd.DataFrame({\n",
    "        'body': [\"Congratulations! You've won a lottery.\", \"Please reply to confirm the meeting.\"],\n",
    "        'extracted_urls': [\"http://lotterywin.com\", \"\"]\n",
    "    })\n",
    "    \n",
    "    # Clean and preprocess the new data\n",
    "    new_data['body'] = new_data['body'].apply(clean_text)\n",
    "    new_data['extracted_urls'] = new_data['extracted_urls'].apply(extract_urls)\n",
    "    \n",
    "    # Predict the label for new data using RandomForest model\n",
    "    model_to_test = models['RandomForest']  # Example: Using RandomForest\n",
    "    predictions = model_to_test.predict(new_data)\n",
    "    \n",
    "    # Output the predictions\n",
    "    print(\"Predictions for the new data:\", predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
