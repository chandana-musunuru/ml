{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b9844-dad4-4340-a0c2-e1c6f44c9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set the NLTK data download path\n",
    "nltk_data_path = 'C:/Users/HP/Desktop/ml/project1 classification/nltk_data'\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('omw-1.4', download_dir=nltk_data_path)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/HP/Desktop/ml/project1 classification/CEAS_08.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_urls(text):\n",
    "   if not isinstance(text, str):\n",
    "       return []\n",
    "   url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "   urls = re.findall(url_pattern, text)\n",
    "   return urls\n",
    "\n",
    "\n",
    "# Function to clean email body\n",
    "def clean_text(text):\n",
    "   if not isinstance(text, str):\n",
    "      text = ''\n",
    "  \n",
    "\n",
    "\n",
    "   text = re.sub(r'http\\S+', '', text)\n",
    "   text = re.sub(r'>+=+=+=+=+', '', text)  # Removing separators like '+=+=+=+=+=+'\n",
    "   text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and digits\n",
    "  \n",
    "   # 3. Lowercase the text\n",
    "   text = text.lower()\n",
    "  \n",
    "   # 4. Remove non-alphabetic characters and numbers\n",
    "   text = re.sub(r'[^a-z\\s]', '', text)\n",
    "  \n",
    "   # 5. Remove stopwords\n",
    "   text_tokens = text.split()\n",
    "   filtered_words = [word for word in text_tokens if word not in stop_words]\n",
    "  \n",
    "   return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "# Apply URL extraction and data cleaning\n",
    "df['extracted_urls'] = df['body'].apply(extract_urls)\n",
    "df['body'] = df['body'].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df[['sender', 'subject', 'body', 'extracted_urls', 'label']].head())\n",
    "\n",
    "\n",
    "# Features and labels\n",
    "X = df[['body', 'extracted_urls']]\n",
    "y = df['label']\n",
    "\n",
    "\n",
    "def message_length(X):\n",
    "   return X.apply(len)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def column_as_string(X):\n",
    "   return X.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"AAA\")\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Preprocessor: handling 'body' and 'extracted_urls' separately\n",
    "preprocessor = ColumnTransformer(\n",
    "   transformers=[\n",
    "       ('body', Pipeline([\n",
    "           ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "           ('tfidf', TfidfVectorizer(min_df=1, max_df=0.9))\n",
    "       ]), 'body'),\n",
    "       ('extracted_urls', Pipeline([\n",
    "           ('convert_to_str', FunctionTransformer(lambda x: x.astype(str), validate=False)),\n",
    "           ('tfidf', TfidfVectorizer())\n",
    "       ]), 'extracted_urls'),\n",
    "        # For 'message_length' column: calculate the length of the message body\n",
    "      ('message_length', Pipeline([\n",
    "           ('length', FunctionTransformer(lambda X: np.array(X.apply(len)).reshape(-1, 1)))  # Reshape to 2D\n",
    "       ]), 'body') \n",
    "   ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create the full pipeline with RandomForestClassifier\n",
    "model = Pipeline([\n",
    "   ('preprocessor', preprocessor),\n",
    "   ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "# Define hyperparameter distributions\n",
    "param_dist = {\n",
    "   'rf__n_estimators': randint(50, 500),  # Use randint for a distribution of values\n",
    "   'rf__max_depth': randint(1, 20)        # Use randint for a distribution of values\n",
    "}\n",
    "\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(model,\n",
    "                                param_distributions=param_dist,\n",
    "                                n_iter=6,\n",
    "                                cv=5,\n",
    "                                random_state=42)  # Added random_state for reproducibility\n",
    "\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = rand_search.predict(X_train)\n",
    "y_pred_test = rand_search.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate your model\n",
    "print(\"Train Set Performance:\")\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "pred_prob_train = rand_search.predict_proba(X_train)[:,1]\n",
    "pred_prob_test = rand_search.predict_proba(X_test)[:,1]\n",
    "\n",
    "\n",
    "# calculate ROC AUC score\n",
    "roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
    "print(\"Test ROC AUC:\", roc_auc_test)\n",
    "\n",
    "\n",
    "# plot the ROC curve\n",
    "fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
    "plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
    "plt.legend()\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# calculate confusion matrix\n",
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11,4))\n",
    "\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[0])\n",
    "ax[0].set_xlabel(\"Predicted Label\")\n",
    "ax[0].set_ylabel(\"True Label\")\n",
    "ax[0].set_title(\"Train Confusion Matrix\")\n",
    "\n",
    "\n",
    "sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"Oranges\", fmt='.4g', ax=ax[1])\n",
    "ax[1].set_xlabel(\"Predicted Label\")\n",
    "ax[1].set_ylabel(\"True Label\")\n",
    "ax[1].set_title(\"Test Confusion Matrix\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "new_email_body = \"\"\"hey mate....\"\"\"\n",
    "\n",
    "\n",
    "# Apply the same preprocessing steps (extract URLs and clean text)\n",
    "new_email_body = clean_text(new_email_body)  # Clean the email body\n",
    "new_email_urls = extract_urls(new_email_body)  # Extract URLs\n",
    "new_email_length = len(new_email_body)  # Calculate the length of the email body\n",
    "\n",
    "\n",
    "# Create a DataFrame similar to the one used for training\n",
    "new_email_df = pd.DataFrame({\n",
    "   'body': [new_email_body],  # Cleaned email body\n",
    "   'extracted_urls': [' '.join(new_email_urls)],  # Join extracted URLs into a single string\n",
    "   'message_length': [new_email_length]  # Include the message length\n",
    "})\n",
    "\n",
    "\n",
    "# Make a prediction using the model pipeline\n",
    "prediction = rand_search.predict(new_email_df)\n",
    "\n",
    "\n",
    "# Print the prediction result\n",
    "if prediction == 0:\n",
    "   print(\"This is not a spam Email!\")\n",
    "else:\n",
    "   print(\"This is a Spam Email!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
